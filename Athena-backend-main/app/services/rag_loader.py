# app/services/rag_loader.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


class AthenaRagPipeline:
    def __init__(self):
        print("üîÑ Loading ATHENA RAG Pipeline...")

        # =============================
        # 1Ô∏è‚É£ Load Embeddings for FAISS
        # =============================
        self.embedding = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # =============================
        # 2Ô∏è‚É£ Load FAISS Index
        # =============================
        self.vectorstore = FAISS.load_local(
            "athena_faiss_index",
            embeddings=self.embedding,
            allow_dangerous_deserialization=True,
        )
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3},
        )

        # =============================
        # 3Ô∏è‚É£ Load Mistral Model
        # =============================
        model_id = "mistralai/Mistral-7B-Instruct-v0.2"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.float16,
        )

        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=250,  # shorter, cleaner answers
            temperature=0.3,      # more stable & educational
            top_p=0.9,
            repetition_penalty=1.1,
        )

        print("‚úÖ ATHENA RAG Pipeline Ready!")

    # =============================
    # Retrieve contextual FAISS knowledge
    # =============================
    def get_context(self, question: str):
        docs = self.retriever.invoke(question)
        if not docs:
            return None
        return "\n\n".join(d.page_content for d in docs)

    # =============================
    # Main reasoning + generation
    # =============================
    def answer(self, question: str, is_new_chat: bool = False):
        context = self.get_context(question)

        # =============================
        # ATHENA Persona
        # =============================
        system_prompt = """
Tu es ATHENA ‚Äî l‚Äôassistante d‚Äôapprentissage intelligente de l‚ÄôESPRIM.
Tu es professionnelle, p√©dagogique, concise et bienveillante.
Tu ne donnes *jamais* de r√©ponses vagues ou g√©n√©riques.
Tu n‚Äôinventes rien hors contexte. Tu expliques clairement.
Tu ne poses pas de questions inutiles.
Tu ne g√©n√®res pas de dialogues artificiels.
Tu ne fais pas de blabla ou de longues introductions.
"""

        # =============================
        # Welcoming message ONLY if:
        # - new conversation
        # - AND question is a greeting
        # =============================
        welcoming = ""
        if is_new_chat or question.lower().strip() in ["hello", "hi", "salut", "hey"]:
            welcoming = "Bonjour ! Comment puis-je t‚Äôaider dans ton apprentissage aujourd‚Äôhui ?\n\n"

        # =============================
        # If no FAISS context found
        # =============================
        if not context:
            context = (
                "Aucun contenu FAISS pertinent n‚Äôa √©t√© trouv√©. "
                "Donne une explication courte, claire et acad√©mique adapt√©e √† un √©tudiant."
            )

        # =============================
        # RAG Prompt Format
        # =============================
        prompt = f"""
{system_prompt}

üìö Contexte extrait :
{context}

‚ùì Question de l‚Äô√©tudiant :
{question}

‚úèÔ∏è R√©ponse d‚ÄôATHENA (claire, concise, structur√©e) :
"""

        # =============================
        # Generate answer
        # =============================
        generated = self.generator(prompt)[0]["generated_text"]

        # Extract clean answer (remove prompt)
        if "R√©ponse d‚ÄôATHENA" in generated:
            answer = generated.split("R√©ponse d‚ÄôATHENA")[-1]
        else:
            answer = generated

        answer = answer.replace(prompt, "").strip()

        # Final cleanup
        answer = answer.replace("üß†", "").replace("üî¥", "")
        answer = answer.replace("R√©ponse :", "").strip()

        # Add welcoming line if needed
        if welcoming:
            answer = welcoming + answer

        return {
            "answer": answer,
            "context_used": context,
        }


# Singleton instance so the model loads only once
athena_rag = AthenaRagPipeline()
